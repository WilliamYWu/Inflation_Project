{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BLS CPI Data Scraper**\n",
    "\n",
    "Description: This will pull Urban Consumer CPI  and LAUS Data from BLS using its json based API.\n",
    "\n",
    "Below are some references for the CPI Data:\n",
    "\n",
    "* [FUll ID CREATION](https://www.bls.gov/help/hlpforma.htm#CU)\n",
    "* [AREA CODES](https://download.bls.gov/pub/time.series/cu/cu.area)\n",
    "* [ITEM CODES](https://download.bls.gov/pub/time.series/cu/cu.item)\n",
    "* [MANUAL DOWNLOAD](https://www.bls.gov/cpi/data.htm)\n",
    "* [API INFORMATION](https://www.bls.gov/developers/api_faqs.htm#register1)\n",
    "\n",
    "Below are some references for the LAUS Data:\n",
    "* [FUll ID CREATION](https://www.bls.gov/help/hlpforma.htm#LA)\n",
    "* [AREA TYPE](https://download.bls.gov/pub/time.series/la/la.area_type)\n",
    "* [AREA CODES](https://download.bls.gov/pub/time.series/la/la.area)\n",
    "* [MEASURE CODE](https://download.bls.gov/pub/time.series/la/la.measure)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Setup\n",
    "\n",
    "1. Packages\n",
    "2. Directories\n",
    "3. Helper Functions\n",
    "4. Setup Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1: Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "import logging.handlers\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: All directories the program used should be included as a global variable here\n",
    "MAIN_DIR =  \"D:\\\\Code\\\\PYTHON\\\\BLS_SCRAPER\\\\\"\n",
    "DATA_DIR = MAIN_DIR + f\"Data\\\\\"\n",
    "\n",
    "CPI_DATA_DIR = DATA_DIR + f\"CPI\\\\\"\n",
    "LAUS_DATA_DIR = DATA_DIR + f\"LAUS\\\\\"\n",
    "FINAL_CPI_DATA_DIR = CPI_DATA_DIR + \"Final\\\\\"\n",
    "FINAL_LAUS_DATA_DIR = LAUS_DATA_DIR + \"Final\\\\\"\n",
    "\n",
    "# NOTE: Automatic Log Folder directory creation based on date.\n",
    "# NOTE: The file iteself is created based on the time. \n",
    "LOG_DIR = MAIN_DIR + f\"Log\\\\{datetime.now().strftime('%Y%m%d')}\\\\\" \n",
    "LOG_FILE = LOG_DIR + f\"Log_{datetime.now().strftime('%H%M%S')}.log\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.3: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory_setup(dir_list):\n",
    "    '''\n",
    "    DESCRIPTION -> If the directory does not exist it will create it\n",
    "    '''\n",
    "    for directory in dir_list:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "def logging_setup():\n",
    "    '''\n",
    "    DESCRIPTION -> Setups the logging file for code\n",
    "    '''\n",
    "    try:\n",
    "      handler = logging.handlers.WatchedFileHandler(os.environ.get(\"LOGFILE\", LOG_FILE))\n",
    "      formatter = logging.Formatter(fmt=\"%(asctime)s %(levelname)-8s %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "      handler.setFormatter(formatter)\n",
    "      logging.getLogger().handlers.clear()\n",
    "      root = logging.getLogger()\n",
    "      root.setLevel(os.environ.get(\"LOGLEVEL\", \"INFO\"))\n",
    "      root.addHandler(handler)\n",
    "      logging.propogate = False\n",
    "      logging.info(\"Log File was created successfully.\")\n",
    "    except Exception as e:\n",
    "        exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: All steps regrading setup should be completed here\n",
    "DIR_LIST = [MAIN_DIR, LOG_DIR, DATA_DIR, CPI_DATA_DIR, LAUS_DATA_DIR, FINAL_CPI_DATA_DIR, FINAL_LAUS_DATA_DIR]\n",
    "directory_setup(DIR_LIST)\n",
    "logging_setup()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: BLS Scraping Class\n",
    "\n",
    "Description: This is the main method used to scrape the information off of BLS.\n",
    "\n",
    "Contains the methods\n",
    "* get_data\n",
    "* process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bls_data_scraper:\n",
    "    '''\n",
    "    ------------------------------------------------------------------------------------------------------------\n",
    "    -----------------------------------------------DESCRIPTION--------------------------------------------------\n",
    "    ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    Passes the BLS json request and gets the data. \n",
    "    Afterwards it processes the data and enriches the data with some additional information about area and item names.\n",
    "\n",
    "    ------------------------------------------------------------------------------------------------------------\n",
    "    -----------------------------------------------PARAMETERS---------------------------------------------------\n",
    "    ------------------------------------------------------------------------------------------------------------\n",
    "    api_key -> API_KEY for BLS data queries\n",
    "    out_file -> Location for Data to be outputted\n",
    "    series_id -> All the series of CPI data that you want\n",
    "    start_year -> Query start range\n",
    "    end_year -> Query end range\n",
    "    area_df -> Dataframe containing information on metro area codes and names\n",
    "    item_df -> Dataframe containing information on item codes and names\n",
    "    cpi_check -> If 1 then this is for CPI. If 0 then this is for LAUS.\n",
    "    '''\n",
    "    def __init__(self, api_key, out_file, series_id, start_year, end_year, area_df, item_df, cpi_check):\n",
    "        headers = {\"Content-type\": \"application/json\"}\n",
    "        parameters = json.dumps({\n",
    "                                \"seriesid\":series_id, \n",
    "                                \"startyear\":start_year, \n",
    "                                \"endyear\":end_year, \n",
    "                                \"registrationkey\":api_key,\n",
    "                                \"calculations\": True\n",
    "                                })\n",
    "        self.area_df = area_df\n",
    "        self.item_df = item_df\n",
    "        self.cpi_check = cpi_check\n",
    "        # Requests the data from BLS\n",
    "        json_data = self.get_data(headers, parameters)\n",
    "        # Processes the data from BLS\n",
    "        df_data = self.process_data(json_data, area_df, item_df, cpi_check)\n",
    "\n",
    "        # Converts the data to an array to write -> Need to do this so that we have a single header\n",
    "        list_df_data = df_data.values.tolist()\n",
    "\n",
    "        # Writes the cleaned up data into the specified out_file\n",
    "        with open(out_file , \"a\") as file:\n",
    "            headers = df_data.columns.tolist()\n",
    "            writer = csv.writer(file, delimiter=',', lineterminator='\\n')\n",
    "            if os.stat(out_file).st_size==0:\n",
    "                writer.writerow(headers)\n",
    "            for row in list_df_data:\n",
    "                writer.writerow(row)\n",
    "\n",
    "\n",
    "    def get_data(self, headers, parameters):\n",
    "        '''\n",
    "        DESCRIPTION -> Posts the url and we get the data back in a json format\n",
    "\n",
    "        PARAM 1 -> headers -> self.header a BLS API requirement\n",
    "        PARAM 2 -> parameters -> The data specification that you plan on querying\n",
    "        '''\n",
    "        post = requests.post('https://api.bls.gov/publicAPI/v2/timeseries/data/', data=parameters, headers=headers)\n",
    "        json_data = json.loads(post.text)\n",
    "        return json_data\n",
    "    \n",
    "\n",
    "    def process_data(self, json_data, area_df, item_df, cpi_check):\n",
    "        '''\n",
    "        DESCRIPTION -> Cleans and enriches the JSON data that we just processed\n",
    "\n",
    "        PARAM 1 -> json_data -> The raw JSON data we pulled from BLS\n",
    "        PARAM 2 -> area_df -> The area code and name information\n",
    "        PARAM 3 -> item_df -> The item code and name information\n",
    "        '''\n",
    "\n",
    "        # NOTE: A lot of the data is stored inside multi-layed dictionaries/lists\n",
    "        # All the information is stored under a three layer depth\n",
    "        df = pd.json_normalize(json_data, record_path=[\"Results\", \"series\", \"data\"], meta=[[\"Results\", \"series\", \"seriesID\"]])\n",
    "        df.rename(columns = {\"Results.series.seriesID\":\"ID\"}, inplace = True)\n",
    "\n",
    "        if cpi_check == 1:\n",
    "            # Parsing out the area_code and item_code from the entirety of the ID that we generated\n",
    "            df[\"area_code\"] = df[\"ID\"].apply(lambda x: x[4:8])\n",
    "            df[\"item_code\"] = df[\"ID\"].apply(lambda x: x[8:])\n",
    "\n",
    "            # Enriching the data here\n",
    "            df = pd.merge(df, area_df, how=\"left\", on=\"area_code\")\n",
    "            df = pd.merge(df, item_df, how=\"left\", on=\"item_code\")\n",
    "            df.drop(columns=[\"area_code\", \n",
    "                             \"item_code\", \n",
    "                             \"footnotes\",\n",
    "                             \"calculations.pct_changes.1\",\n",
    "                             \"calculations.pct_changes.3\",\n",
    "                             \"calculations.pct_changes.6\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "            # # rearrange column ordering\n",
    "            # name_list = df.columns.tolist()\n",
    "            # name_list = name_list[-3:-2] + name_list[-2:-1] + name_list[-1:] + name_list[:-3]\n",
    "            # df = df[name_list]\n",
    "        else:\n",
    "            # We don't need the calculation net changes for LAUS Data\n",
    "            df[\"area_code\"] = df[\"ID\"].apply(lambda x: x[3:18])\n",
    "            df = pd.merge(df, area_df, how=\"left\", on=\"area_code\")\n",
    "            df.drop(columns=[\"latest\", \n",
    "                             \"footnotes\", \n",
    "                             \"period\", \n",
    "                             \"type_code\", \n",
    "                             \"area_name\", \n",
    "                             \"calculations.net_changes.1\",\n",
    "                             \"calculations.net_changes.3\",\n",
    "                             \"calculations.net_changes.6\",\n",
    "                             \"calculations.net_changes.12\",\n",
    "                             \"calculations.pct_changes.1\",\n",
    "                             \"calculations.pct_changes.3\",\n",
    "                             \"calculations.pct_changes.6\",\n",
    "                             \"calculations.pct_changes.12\"], inplace=True, errors=\"ignore\")\n",
    "        return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: BLS CPI Area and Item & LAUS Area Information Scraper\n",
    "\n",
    "1. bls_code_scraper function \n",
    "2. Get and Filter the Information\n",
    "3. Match the Filtered Codes to LAUS Data\n",
    "4. Generate List of BLS Codes from Filtered Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1: Scrape BLS ID Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bls_id_scraper(url, file_dir):\n",
    "    '''\n",
    "    DESCRIPTION -> Scrapes textual information off of BLS links for CPI and LAUS\n",
    "\n",
    "    PARAM 1 -> file_dir -> Where you want to write the raw_information\n",
    "    '''\n",
    "\n",
    "    response = requests.get(url)\n",
    "    content = response.content.decode(\"utf-8\")\n",
    "    area_content = csv.reader(content.splitlines(), delimiter=\"\\t\")\n",
    "    area_list = list(area_content)\n",
    "\n",
    "    # n is the number of elements that we want to remove from the back of the list\n",
    "    n = 3\n",
    "    # Writing the uncleaned relavant information into file \n",
    "    with open(file_dir, \"w\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in area_list:\n",
    "            row = row[: len(row) - n]\n",
    "            writer.writerow(row)\n",
    "    file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2: Get and Filter the Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the CPI area code information\n",
    "bls_id_scraper(\"https://download.bls.gov/pub/time.series/cu/cu.area\", f\"{CPI_DATA_DIR}\\\\raw_area.csv\")\n",
    "# Scraping the CPI item code information\n",
    "bls_id_scraper(\"https://download.bls.gov/pub/time.series/cu/cu.item\", f\"{CPI_DATA_DIR}\\\\raw_item.csv\")\n",
    "# Scraping the LAUS area code information\n",
    "bls_id_scraper(\"https://download.bls.gov/pub/time.series/la/la.area\", f\"{LAUS_DATA_DIR}\\\\raw_area.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpi_area_df = pd.read_csv(f\"{CPI_DATA_DIR}\\\\raw_area.csv\")\n",
    "cpi_item_df = pd.read_csv(f\"{CPI_DATA_DIR}\\\\raw_item.csv\")\n",
    "\n",
    "\"\"\"\n",
    "------------------------------AREA FILTERS-----------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "Filter: Filter for S area_code, since we only want metro area codes\n",
    "'''\n",
    "cpi_area_df[\"code_check\"] = cpi_area_df[\"area_code\"].apply(lambda x: \"Pass\" if re.match(\"S[A-Z0-9]{3}\", x) else \"Fail\")\n",
    "cpi_area_df = cpi_area_df[cpi_area_df[\"code_check\"] == \"Pass\"]\n",
    "\n",
    "'''\n",
    "Filter: Remove the area_names that have Size Class A in their name. Those are irrelavent.\n",
    "'''\n",
    "cpi_area_df[\"name_check\"] = cpi_area_df[\"area_name\"].apply(lambda x: \"Pass\" if \"Size Class A\" not in x else \"Fail\")\n",
    "cpi_area_df = cpi_area_df[cpi_area_df[\"name_check\"] == \"Pass\"]\n",
    "\n",
    "\"\"\"\n",
    "------------------------------ITEM FILTERS-----------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "Filter: Filter that selects only those at the aggregate: all items, all items less food, and all items less food and energy\n",
    "'''\n",
    "cpi_item_df[\"code_check\"] = cpi_item_df[\"item_code\"].apply(lambda x: \"Pass\" if re.match(\"SA0[^R;]*\", x) else \"Fail\")\n",
    "cpi_item_df = cpi_item_df[cpi_item_df[\"code_check\"] == \"Pass\"]\n",
    "\n",
    "'''\n",
    "Filter: Only include item_codes that have a shorter length than 4 since those are the more aggregated values.\n",
    "'''\n",
    "# cpi_item_df[\"code_check\"] = cpi_item_df[\"item_code\"].apply(lambda x: \"Pass\" if len(x)<4 else \"Fail\")\n",
    "# cpi_item_df = cpi_item_df[cpi_item_df[\"code_check\"] == \"Pass\"]\n",
    "\n",
    "'''\n",
    "Filter: If the item_name contains All Items we remove since those are too aggregated\n",
    "'''\n",
    "# cpi_item_df[\"name_check\"] = cpi_item_df[\"item_name\"].apply(lambda x: \"Pass\" if not re.match(\"All items [a-zA-Z\\s,]*\",x) else \"Fail\")\n",
    "# cpi_item_df = cpi_item_df[cpi_item_df[\"name_check\"] == \"Pass\"]\n",
    "\n",
    "cpi_area_df.drop(columns=[\"code_check\", \"name_check\"], inplace=True)\n",
    "cpi_item_df.drop(columns=[\"code_check\"], inplace=True)\n",
    "\n",
    "cpi_area_df.to_csv(f\"{CPI_DATA_DIR}\\\\clean_area.csv\", index=False)\n",
    "cpi_item_df.to_csv(f\"{CPI_DATA_DIR}\\\\clean_item.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.3: Match the Filtered Codes to LAUS Data\n",
    "\n",
    "Depending on the remaining codes in area code we will create their respective LAUS codes based on the pre-comma area_text/area_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: Further processing is needed for the cpi_area_df. There are discrepancies between the identifier in that and whats available in the cpi_area_df\n",
    "# WARNING: Check on Boston later it might have a parsing issue as well\n",
    "cpi_area_df[\"laus_check\"] = cpi_area_df[\"area_name\"].apply(lambda x: x.split(\",\")[0].replace(\" \",\"\"))\n",
    "\n",
    "'''\n",
    "Filter: Remove the urban tag for both Hawaii and Alaska ex.) Urban Hawaii -> Hawaii\n",
    "'''\n",
    "cpi_area_df[\"laus_check\"] = cpi_area_df[\"laus_check\"].apply(lambda x: x.replace(\"Urban\",\"\") if re.match(\"Urban[A-Za-z]+\", x) else x)\n",
    "\n",
    "'''\n",
    "Filter: Remove WA from the filter, since it didn't split properly without the comma that was supposed to be there. Thank you BLS.\n",
    "'''\n",
    "cpi_area_df[\"laus_check\"] = cpi_area_df[\"laus_check\"].apply(lambda x: x.replace(\"WA\",\"\") if re.match(\"Seattle-Tacoma-BellevueWA\", x) else x)\n",
    "\n",
    "laus_area_df = pd.read_csv(f\"{LAUS_DATA_DIR}\\\\raw_area.csv\")\n",
    "# Create the same format laus_check as what you did for the cpi_area_df so that you can merge it.\n",
    "laus_area_df[\"laus_check\"] = laus_area_df[\"area_text\"].apply(lambda x: x.split(\",\")[0].replace(\" \", \"\"))\n",
    "laus_merge_df = pd.merge(cpi_area_df, laus_area_df, how=\"left\", on=\"laus_check\")\n",
    "\n",
    "laus_merge_df.drop(columns=[\"area_code_x\", \"area_name\",], inplace=True)\n",
    "laus_merge_df.rename(columns={\"area_type_code\":\"type_code\", \"area_code_y\":\"area_code\", \"area_text\":\"area_name\"}, inplace=True)\n",
    "\n",
    "'''\n",
    "Filter: Removing those that have an area_type other than B Metropolitan Areas\n",
    "'''\n",
    "laus_merge_df.drop(laus_merge_df[laus_merge_df[\"type_code\"] != \"B\"].index, inplace=True)\n",
    "\n",
    "laus_merge_df.to_csv(f\"{LAUS_DATA_DIR}\\\\clean_area.csv\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.4: Generate List of BLS CPI Codes from Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here is a sample of how the BLS creates the unique identifier. You can find more information on it here (https://www.bls.gov/help/hlpforma.htm#id=CU).\n",
    "\n",
    "\tSeries ID    CUUR0000SA0L1E\n",
    "\tPositions       Value           Field Name\n",
    "\t1-2             CU              Prefix\n",
    "\t3               U               Not Seasonal Adjustment Code\n",
    "\t4               R               Periodicity Code\n",
    "\t5-8             0000            Area Code\n",
    "\t9               S               Base Code\n",
    "\t10-16           A0L1E           Item Code\n",
    "\"\"\"\n",
    "cpi_code_list = []\n",
    "# Consumer Price Index - All Urban Consumers\n",
    "prefix = \"CU\"\n",
    "# This dataset does not have any seasonally adjusted data\n",
    "seasonality = \"U\"\n",
    "# R stands for monthly\n",
    "periodicity = \"R\"\n",
    "for area in cpi_area_df[\"area_code\"]:\n",
    "    for item in cpi_item_df[\"item_code\"]:\n",
    "        cpi_code_list.append(f\"{prefix}{seasonality}{periodicity}{str(area)}{str(item)}\")\n",
    "logging.info(f\"Total Unique CPI Codes: {len(cpi_code_list)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.5: Generate List of BLS LAUS Codes from Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\tSeries ID    LAUCN281070000000003\n",
    "\tPositions    Value            Field Name\n",
    "\t1-2          LA               Prefix\n",
    "\t3            U                Seasonal Adjustment Code\n",
    "\t4-18         CN2810700000000  Area Code\n",
    "\t19-20        03               Measure Code\n",
    "'''\n",
    "\n",
    "laus_code_list = []\n",
    "# Local Area Unemployment\n",
    "prefix = \"LA\"\n",
    "# Seasonality Adjustment\n",
    "seasonality = \"U\"\n",
    "# area code and measure code\n",
    "measure_code = \"03\"\n",
    "\n",
    "laus_code_list = []\n",
    "for laus_area_code in laus_merge_df[\"area_code\"]:\n",
    "    laus_code_list.append(f\"{prefix}{seasonality}{laus_area_code}{measure_code}\")\n",
    "logging.info(f\"Total Unique LAUS Codes: {len(laus_code_list)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Main\n",
    "\n",
    "1. Setup For API Calls\n",
    "2. Function for Checking File Existence\n",
    "3. BLS CPI API Call \n",
    "4. BLS LAUS API Call\n",
    "5. Merging Our Datasets Together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.1: Setup For API Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "--------- KEY FEATURES/LIMITATIONS OF API ----------\n",
    "Limit of 500 calls per key.\n",
    "Can make 50 calls every query.\n",
    "20 years per query\n",
    "'''\n",
    "# Drexel API Key: \"024f5a0ca6e7494cbec2ea4088cd4a9d\"\n",
    "# GMAIL API Key: \"73df4bb81189431089fe2f247af35ce1\"\n",
    "api_key = \"024f5a0ca6e7494cbec2ea4088cd4a9d\"\n",
    "start_year_1 = 1990\n",
    "end_year_1 = 2005\n",
    "start_year_2 = 2006\n",
    "end_year_2 = 2022"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.2: Function for Checking File Existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_existance(file_paths):\n",
    "    for file_path in file_paths:\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                logging.info(\"Removed old file for data.\")\n",
    "            except:\n",
    "                logging.info(\"Did not remove old file for data.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.3: BLS CPI API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-----------------BLS CPI API CALLS HERE--------------------\n",
    "'''\n",
    "\n",
    "file_existance([f\"{FINAL_CPI_DATA_DIR}\\\\bls_cpi_data_p1.csv\", f\"{FINAL_CPI_DATA_DIR}\\\\bls_cpi_data_p2.csv\"])\n",
    "for x in range(0, len(cpi_code_list), 50):\n",
    "    code_chunk = cpi_code_list[x:x+50]\n",
    "    bls_data_scraper(api_key, f\"{FINAL_CPI_DATA_DIR}\\\\bls_cpi_data_p1.csv\", code_chunk, start_year_1, end_year_1, cpi_area_df, cpi_item_df,1)\n",
    "    bls_data_scraper(api_key, f\"{FINAL_CPI_DATA_DIR}\\\\bls_cpi_data_p2.csv\", code_chunk, start_year_2, end_year_2, cpi_area_df, cpi_item_df,1)\n",
    "    time.sleep(2)\n",
    "logging.info(\"Done with BLS CPI API Calls\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.4: BLS LAUS API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_existance([f\"{FINAL_LAUS_DATA_DIR}\\\\bls_laus_data_p1.csv\", f\"{FINAL_LAUS_DATA_DIR}\\\\bls_laus_data_p2.csv\"])\n",
    "# NOTE: Technically we don't need to make an exception for the LAUS dataset. However, since we only have one item this just makes life easier. \n",
    "# NOTE: If we wanted to though, this could just be one general method that works for both cases.\n",
    "for x in range(0, len(laus_code_list), 50):\n",
    "    code_chunk = laus_code_list[x:x+50]\n",
    "    bls_data_scraper(api_key, f\"{FINAL_LAUS_DATA_DIR}\\\\bls_laus_data_p1.csv\", code_chunk, start_year_1, end_year_1, laus_merge_df,0,0)\n",
    "    bls_data_scraper(api_key, f\"{FINAL_LAUS_DATA_DIR}\\\\bls_laus_data_p2.csv\", code_chunk, start_year_2, end_year_2, laus_merge_df,0,0)\n",
    "    time.sleep(2)\n",
    "logging.info(\"Done with BLS LAUS API Calls\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.5: Merging Our Datasets Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpi_merge = pd.concat([pd.read_csv(f\"{FINAL_CPI_DATA_DIR}\\\\bls_cpi_data_p1.csv\"), pd.read_csv(f\"{FINAL_CPI_DATA_DIR}\\\\bls_cpi_data_p2.csv\")])\n",
    "laus_merge = pd.concat([pd.read_csv(f\"{FINAL_LAUS_DATA_DIR}\\\\bls_laus_data_p1.csv\"), pd.read_csv(f\"{FINAL_LAUS_DATA_DIR}\\\\bls_laus_data_p2.csv\")])\n",
    "\n",
    "cpi_merge.to_csv(f\"{FINAL_CPI_DATA_DIR}\\\\bls_cpi_data_cmb.csv\", index=False)\n",
    "laus_merge.to_csv(f\"{FINAL_LAUS_DATA_DIR}\\\\bls_laus_data_cmb.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85fec92199cc14a2bd4b3d9b9709b8077b8d4ba9daddeb167bcfeac63be00291"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
