{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "import logging.handlers\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: All directories the program used should be included as a global variable here\n",
    "MAIN_DIR =  \"D:\\\\Code\\\\PYTHON\\\\BLS_SCRAPER\\\\\"\n",
    "DATA_DIR = MAIN_DIR + f\"Data\\\\\"\n",
    "\n",
    "CPI_DATA_DIR = DATA_DIR + f\"CPI\\\\\"\n",
    "LAUS_DATA_DIR = DATA_DIR + f\"LAUS\\\\\"\n",
    "FINAL_CPI_DATA_DIR = CPI_DATA_DIR + \"Final\\\\\"\n",
    "FINAL_LAUS_DATA_DIR = LAUS_DATA_DIR + \"Final\\\\\"\n",
    "\n",
    "# NOTE: Automatic Log Folder directory creation based on date.\n",
    "# NOTE: The file iteself is created based on the time. \n",
    "LOG_DIR = MAIN_DIR + f\"Log\\\\{datetime.now().strftime('%Y%m%d')}\\\\\" \n",
    "LOG_FILE = LOG_DIR + f\"Log_{datetime.now().strftime('%H%M%S')}.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory_setup(dir_list):\n",
    "    '''\n",
    "    DESCRIPTION -> If the directory does not exist it will create it\n",
    "    '''\n",
    "    for directory in dir_list:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "def logging_setup():\n",
    "    '''\n",
    "    DESCRIPTION -> Setups the logging file for code\n",
    "    '''\n",
    "    try:\n",
    "      handler = logging.handlers.WatchedFileHandler(os.environ.get(\"LOGFILE\", LOG_FILE))\n",
    "      formatter = logging.Formatter(fmt=\"%(asctime)s %(levelname)-8s %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "      handler.setFormatter(formatter)\n",
    "      logging.getLogger().handlers.clear()\n",
    "      root = logging.getLogger()\n",
    "      root.setLevel(os.environ.get(\"LOGLEVEL\", \"INFO\"))\n",
    "      root.addHandler(handler)\n",
    "      logging.propogate = False\n",
    "      logging.info(\"Log File was created successfully.\")\n",
    "    except Exception as e:\n",
    "        exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: All steps regrading setup should be completed here\n",
    "DIR_LIST = [MAIN_DIR, LOG_DIR, DATA_DIR, CPI_DATA_DIR, LAUS_DATA_DIR, FINAL_CPI_DATA_DIR, FINAL_LAUS_DATA_DIR]\n",
    "directory_setup(DIR_LIST)\n",
    "logging_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\tSeries ID    LAUCN281070000000003\n",
    "\tPositions    Value            Field Name\n",
    "\t1-2          LA               Prefix\n",
    "\t3            U                Seasonal Adjustment Code\n",
    "\t4-18         CN2810700000000  Area Code\n",
    "\t19-20        03               Measure Code\n",
    "'''\n",
    "laus_merge_df = pd.read_csv(f\"{LAUS_DATA_DIR}\\\\clean_area.csv\")\n",
    "laus_code_list = []\n",
    "# Local Area Unemployment\n",
    "prefix = \"LA\"\n",
    "# Seasonality Adjustment\n",
    "seasonality = \"U\"\n",
    "# area code and measure code\n",
    "measure_code = \"03\"\n",
    "\n",
    "laus_code_list = []\n",
    "for laus_area_code in laus_merge_df[\"area_code\"]:\n",
    "    laus_code_list.append(f\"{prefix}{seasonality}{laus_area_code}{measure_code}\")\n",
    "logging.info(f\"Total Unique LAUS Codes: {len(laus_code_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laus_check    object\n",
      "type_code     object\n",
      "area_code     object\n",
      "area_name     object\n",
      "dtype: object\n",
      "year          object\n",
      "period        object\n",
      "periodName    object\n",
      "latest        object\n",
      "value         object\n",
      "footnotes     object\n",
      "ID            object\n",
      "area_code     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "class bls_data_scraper:\n",
    "    '''\n",
    "    ------------------------------------------------------------------------------------------------------------\n",
    "    -----------------------------------------------DESCRIPTION--------------------------------------------------\n",
    "    ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    Passes the BLS json request and gets the data. \n",
    "    Afterwards it processes the data and enriches the data with some additional information about area and item names.\n",
    "\n",
    "    ------------------------------------------------------------------------------------------------------------\n",
    "    -----------------------------------------------PARAMETERS---------------------------------------------------\n",
    "    ------------------------------------------------------------------------------------------------------------\n",
    "    api_key -> API_KEY for BLS data queries\n",
    "    out_file -> Location for Data to be outputted\n",
    "    series_id -> All the series of CPI data that you want\n",
    "    start_year -> Query start range\n",
    "    end_year -> Query end range\n",
    "    area_df -> Dataframe containing information on metro area codes and names\n",
    "    item_df -> Dataframe containing information on item codes and names\n",
    "    cpi_check -> If 1 then this is for CPI. If 0 then this is for LAUS.\n",
    "    '''\n",
    "    def __init__(self, api_key, out_file, series_id, start_year, end_year, area_df, item_df, cpi_check):\n",
    "        headers = {\"Content-type\": \"application/json\"}\n",
    "        parameters = json.dumps({\n",
    "                                \"seriesid\":series_id, \n",
    "                                \"startyear\":start_year, \n",
    "                                \"endyear\":end_year, \n",
    "                                \"registrationkey\":api_key\n",
    "                                })\n",
    "        self.area_df = area_df\n",
    "        self.item_df = item_df\n",
    "        self.cpi_check = cpi_check\n",
    "        # Requests the data from BLS\n",
    "        json_data = self.get_data(headers, parameters)\n",
    "        # Processes the data from BLS\n",
    "        df_data = self.process_data(json_data, area_df, item_df, cpi_check)\n",
    "\n",
    "        # Converts the data to an array to write -> Need to do this so that we have a single header\n",
    "        list_df_data = df_data.values.tolist()\n",
    "\n",
    "        # Writes the cleaned up data into the specified out_file\n",
    "        with open(out_file , \"a\") as file:\n",
    "            headers = df_data.columns.tolist()\n",
    "            writer = csv.writer(file, delimiter=',', lineterminator='\\n')\n",
    "            if os.stat(out_file).st_size==0:\n",
    "                writer.writerow(headers)\n",
    "            for row in list_df_data:\n",
    "                writer.writerow(row)\n",
    "\n",
    "\n",
    "    def get_data(self, headers, parameters):\n",
    "        '''\n",
    "        DESCRIPTION -> Posts the url and we get the data back in a json format\n",
    "\n",
    "        PARAM 1 -> headers -> self.header a BLS API requirement\n",
    "        PARAM 2 -> parameters -> The data specification that you plan on querying\n",
    "        '''\n",
    "        post = requests.post('https://api.bls.gov/publicAPI/v2/timeseries/data/', data=parameters, headers=headers)\n",
    "        json_data = json.loads(post.text)\n",
    "        return json_data\n",
    "    \n",
    "\n",
    "    def process_data(self, json_data, area_df, item_df, cpi_check):\n",
    "        '''\n",
    "        DESCRIPTION -> Cleans and enriches the JSON data that we just processed\n",
    "\n",
    "        PARAM 1 -> json_data -> The raw JSON data we pulled from BLS\n",
    "        PARAM 2 -> area_df -> The area code and name information\n",
    "        PARAM 3 -> item_df -> The item code and name information\n",
    "        '''\n",
    "\n",
    "        # NOTE: A lot of the data is stored inside multi-layed dictionaries/lists\n",
    "        # All the information is stored under a three layer depth\n",
    "        df = pd.json_normalize(json_data, record_path=[\"Results\", \"series\", \"data\"], meta=[[\"Results\", \"series\", \"seriesID\"]])\n",
    "        df.rename(columns = {\"Results.series.seriesID\":\"ID\"}, inplace = True)\n",
    "\n",
    "        if cpi_check == 1:\n",
    "            # Parsing out the area_code and item_code from the entirety of the ID that we generated\n",
    "            df[\"area_code\"] = df[\"ID\"].apply(lambda x: x[4:8])\n",
    "            df[\"item_code\"] = df[\"ID\"].apply(lambda x: x[8:])\n",
    "\n",
    "            # Enriching the data here\n",
    "            df = pd.merge(df, area_df, how=\"left\", on=\"area_code\")\n",
    "            df = pd.merge(df, item_df, how=\"left\", on=\"item_code\")\n",
    "            df.drop(columns=[\"area_code\", \"item_code\", \"footnotes\"], inplace=True)\n",
    "\n",
    "            # # rearrange column ordering\n",
    "            # name_list = df.columns.tolist()\n",
    "            # name_list = name_list[-3:-2] + name_list[-2:-1] + name_list[-1:] + name_list[:-3]\n",
    "            # df = df[name_list]\n",
    "        else:\n",
    "            \n",
    "            df[\"area_code\"] = df[\"ID\"].apply(lambda x: x[3:18])\n",
    "            df = pd.merge(df, area_df, how=\"outer\", on=\"area_code\")\n",
    "            df.drop(columns=[\"latest\", \"footnotes\", \"period\", \"type_code\", \"area_name\"], inplace=True)\n",
    "        return df\n",
    "    \n",
    "    '''\n",
    "-----------------BLS LAUS API CALLS HERE--------------------\n",
    "'''\n",
    "# Drexel API Key: \"024f5a0ca6e7494cbec2ea4088cd4a9d\"\n",
    "# GMAIL API Key: \"73df4bb81189431089fe2f247af35ce1\"\n",
    "api_key = \"73df4bb81189431089fe2f247af35ce1\"\n",
    "start_year = 2010\n",
    "end_year = 2022\n",
    "if os.path.exists(f\"{FINAL_LAUS_DATA_DIR}\\\\bls_laus_data.csv\"):\n",
    "    try:\n",
    "        os.remove(f\"{FINAL_LAUS_DATA_DIR}\\\\bls_laus_data.csv\")\n",
    "        logging.info(\"Removed old file for data.\")\n",
    "    except:\n",
    "            logging.info(\"Did not remove old file for data.\")\n",
    "\n",
    "# NOTE: Technically we don't need to make an exception for the LAUS dataset. However, since we only have one item this just makes life easier. \n",
    "# NOTE: If we wanted to though, this could just be one general method that works for both cases.\n",
    "for x in range(0, len(laus_code_list), 50):\n",
    "    code_chunk = laus_code_list[x:x+50]\n",
    "    bls_data_scraper(api_key, f\"{FINAL_LAUS_DATA_DIR}\\\\bls_laus_data.csv\", code_chunk, start_year, end_year, laus_merge_df,0,0)\n",
    "    time.sleep(2)\n",
    "logging.info(\"Done with BLS LAUS API Calls\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85fec92199cc14a2bd4b3d9b9709b8077b8d4ba9daddeb167bcfeac63be00291"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
